{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprezentatywnosc danych\n",
    "\n",
    "\n",
    "Glownym celem analizy zmiennych jest ocena ich przydatnosci. Statystyki opisowe, informacje o brakujacych danych, ilosc informacji ukrytych w zmiennych oraz histogramy przedstawiajace rozklady wartosci pozwalajacych na wczesnym etapie projektu wykryc i udokumentowac problemy z danymi. Problemy z jakoscia danych wystepuja zawsze, a niezauwazone moga doprowadzic do fiska calego projektu albo co najmniej znacznie go skomplikowac.\n",
    "\n",
    "Znajac tendencje centralna, rozproszenie i rozklad wartosci, mozemy ocenic, czy sa one zgodne z oczekiwaniami.\n",
    "\n",
    "Oceniajac reprezentatywnosc proby nalezy uzyskac odpowiedzi na ponizsze pytania:\n",
    "\n",
    "- Jak zostaly wybrane do niej przyklady?\n",
    "- Czy wybrane przyklady sa w jakis sposob wyjatkowe?\n",
    "\n",
    "# Korelacje miedzy zmiennymi\n",
    "\n",
    "Dwie zmienne sa ze soba skorelowane jezeli, znajomosc wartosci jednej zmiennej pomaga, chocby w niewielkim stopniu, w okresleniu wartosci drugiej zmiennej. Zmienna wyjsciowa, inaczej objasniana, nazywa sie tez zalezna. Oznacza to, ze zmienna wyjsciowa powinna byc skorelowana ze zmiennymi wejsciowymi.\n",
    "\n",
    "\n",
    "# Ocena przydatnosci zmiennych\n",
    "\n",
    "Najpopularniejszym wspolczynnikiem korelacji zmiennych numerycznych jest wspolczynnik korelacji liniowej r-Pearsona. Jest to test parametryczny polegajacy na podzieleniu kowarjancji zmiennych przez iloczyn ich srednich:\n",
    "\n",
    "$$r_{XY} = \\frac{cov(X,Y)}{\\sigma X \\cdot \\sigma Y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obliczanie korelacji zmiennych numerycznych\n",
    "df = pd.DataFrame()\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korelacje miedzy zmiennymi kategorycznymi mozna obliczyc na podstwie tabel czestosci. Tym razem jednak interesuje nas warunkowy rozklad zmiennych, a wiec policzymy, jak czesto wystepowaly wartosci X dla poszczegolnych wartosci Y.\n",
    "\n",
    "\n",
    "```python\n",
    "survived_sex = pd.crosstab(index=data[\"Survived\"], columns=data[\"Sex\"])\n",
    "survived_sex.index = [\"died\", \"survived\"]\n",
    "print(survived_sex)\n",
    "\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(pd.crosstab(data.Sex, data.Survived))\n",
    "print('Statistics')\n",
    "alpha = 0.05\n",
    "\n",
    "if p > alpha:\n",
    "    print(\"Variables seems to be independent\")\n",
    "else:\n",
    "    print(\"Variables seems to be dependent\")\n",
    "```\n",
    "\n",
    "### Dwie zmienne porzadkowe\n",
    "\n",
    "Do oceny korelacji miedzy kategorycznymi zmiennymi porzadkowymi mozna uzyc testu rho-Spearmana. Zaklada on jednak, ze rangi sa liczbami calkowitymi, czyli powinien byc uzywany do oceny korelacji zmiennych porzadkowych mierzonych na dlugich skalach.\n",
    "\n",
    "\n",
    "```python\n",
    "coef, p = st.spearmanr(data.Survived, data.Pclass)\n",
    "print(f'Statistics {coef} {p}')\n",
    "\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print(\"Variables seems to be independent\")\n",
    "else:\n",
    "    print(\"Variables seems to be dependent\")\n",
    "\n",
    "```\n",
    "\n",
    "Jezeli skale sa krotkie to, wystepuje wiele rang wiazanych i wtedy bardziej odpowiednim wspolczynnikiem jest tau-Kendalla.\n",
    "\n",
    "```python\n",
    "coef, p = st.kendalltau(data.Survived, data.Pclass)\n",
    "print(f\"Statistics {coef} {p}\")\n",
    "alpha = 0.05\n",
    "\n",
    "if p > alpha:\n",
    "    print(\"Variables seems to be independent\")\n",
    "else:\n",
    "    print(\"Variables seems to be dependent\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zmienna kategoryczna i numeryczna\n",
    "\n",
    "Korelacje miedzy zmienna numeryczna a kategoryczna mozna ocenic, przeprowadzajac analize wariancje ANOVA. Polega ona na znalezieniu stosunku wariancji obliczonej miedzy badanymi grupami a srednia wariancja zaobserwowana wewnatrz grup. Hipoteza zerowa zaklada, ze jedynym zrodlem wariancji jest wariancja zaobserwowana wewnatrz grup, czyli ze wariancja miedzy grupami ma zerowy wplyw. Do weryfikacji hipotezy nalezy obliczyc srednie kwadraty.\n",
    "\n",
    "Stosunek srednich kwadratow obliczanych dla poszczegolnych stanow zmiennej kategorycznej w odniesieniu do srednich kwadratow bledu pozwala oszacowac wplyw kazdego z tych stanow oddzielenie na wartosci zmiennej numerycznej.\n",
    "\n",
    "Jednoczynnikowa analiza wariancji pozwala ocenic wplyw poszczegolnych stanow zmiennej kategorycznej na zmienna numeryczna. Dzielac policzona wartosc przez srednie kwadraty, otrzymamy statystyke F.\n",
    "\n",
    "Okresla ona, czy poszczegolne stany zmiennej kategorycznej maja wplyw na zmienna numeryczna. Im wieksza wartosc F, z tym wieksza pewnosc mozemy odrzucic hipoteze zerowa. Otrzymany wynik nalezy sprawdzic w tablicach rozkladu statystki $F$. Zawieraja one wartosci punktow krytycznych dla okreslonych stopni swobody DF.\n",
    "\n",
    "```python\n",
    "mod = ols('Age ~ Sex', data=data).fit()\n",
    "aov_table = sm.stats.anova_lm(mod, typ=2)\n",
    "print(aov_table)\n",
    "```\n",
    "\n",
    "Prawdopodobienstwo ze zmienne sa niezalezne znajduje sie w kolumnie PR(>F). Typowy punkt krytyczny wynosi 0.05 wiec jezeli prawdopodobienstwo jest wiekrze oznacza ze zmienne sa zalezne."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0394993702810e63cf4a2cb4306f486c4e50bf3299d5af3d2bf38c01a54e229"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie danych\n",
    "\n",
    "## Uporzadkowanie danych\n",
    "\n",
    "W modelu relacyjnym dane trzymane sa przechowywane w wielu odrebnych tabelach. Te powiazania laczace tabele nazywa sie relacjami. Aby relacje byly jednoznaczne tabele maja klucze podstawowe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podstawowe probelmy w uporzadkowywaniu danych\n",
    "\n",
    "- Pierwszy problem polega na przechowywaniu w roznych komorkach wartosci wielu zmiennych.\n",
    "- Drugim typowym problemem jest przechowywanie w jedenj kolumnie polaczonych wartosci wielu zmiennych. Nie ma prostego i wydajnego sposobu analizy takich danych.\n",
    "- Trzecim typowym problemem jest podzielenie wartosci tej samej zmiennej miedzy rozne kolumny.\n",
    "- Czwarty typowy problem wynika z uzywania jaki naglowkow kolumn wartosci zamiast nazw zmiennych.\n",
    "- Piaty typowy problem polega na podzieleniu przykladow miedzy wiele tabel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przeksztalcanie danych\n",
    "\n",
    "Skutecznosc roznych metod optymalizacji procesu uczenia oraz jakosci wynikow poszczegolnych funkcji kosztu silnie zaleza od danych treningowych. Na przyklad proces uczenia metoda najszybszego spadku gradientu jest najbardziej efektywny dla danych, ktorych srednia wynosi 0. Inny przyklad - metody podzialu drzew decyzyjnych faworyzuja zmienne kategoryczne o wiekrzej liczbie stanow. Z kolei funkcji minimalizujaca kwadraty bledu L2 jest bardzo wrazliwa na dane odstajace.\n",
    "\n",
    "## Zaokroglanie\n",
    "\n",
    "Zaokraglic nalezy zmienne, ktore podejrzewamy o wyrazenie w na tyle malej jednostce, ze ich wartosci sa obarczone duzym ryzykiem bledu pomiaru.\n",
    "\n",
    "## Dyskretyzacja\n",
    "\n",
    "Redukcja liczby wartosci zmiennej numerycznej polega na jej przeksztalceniu w zmienna kategoryczna porzadkowa. Dyskretyzacje mozna przeprowadzic na rozne sposoby. Jedny z nich jest podzial zmiennej na przedzialy stalej szerokosci, staramy sie zachowac rozklad zmiennej. Podzial na przedzialy o takiej samej liczbie przypadkow ma na celu zachowanie jej entropii, a podzial na przedzialy o zdefiniowanych wartosciach sluzy do wprowadzenia do modelu wiedzy z dziedziny eksperymentu.\n",
    "\n",
    "## Skalowanie\n",
    "\n",
    "Skalowanie metoda Min-Max, czyli normalizacja. Polega ona na obliczaniu wartosci minimalnej i maksymalnej, a nastepnie odcieciu od oryginalnej wartosci minimum i podzieleniu wyniku przez rozstep. Otrzymamy w ten sposob wartosc z zakresu 0 - 1. Skalowanie metoda Min-Max jest wrazliwe na wartosci odstajace. Jezeli one wystepuja nalezy uzyc statystyki niewrazliwej na wartosci odstajace, czyli rozstepu cwiartkowego. Skalowanie IQR polega na zastapieniu wartosci najmniejszej pierwszym kwartylem, a rozstepu - rozstepem cwiartkowym.\n",
    "\n",
    "$$X_{robust} = \\frac{X - Q1(X)}{[Q3(X) - Q1(X)]} $$\n",
    "\n",
    "Istnieja rowniez takie funkcje skalujace jak softmax oraz skalowanie logarytmiczne.\n",
    "\n",
    "## Wygladzenie\n",
    "\n",
    "Szeregi czasowe zawieraja trzy skladowe: trend, cykl i szum. Trend reprezentuje monotoniczny skladnik serii danych, czyli rosnacy, malejacy i staly. Cykl reprezentuje okresowe zmiany wartosci. Szum jest tym, co pozostaje po usunieciu z szeregu czasowego trendu i cyklu.\n",
    "\n",
    "Najbardziej powszechna technika wygladzenia polega na uzyciu sredniej ruchomej. Rodzaje sredniej ruchomej:\n",
    "\n",
    "- Prosta srednia ruchoma\n",
    "- Wazona srednia ruchoma uwzglednia wiekrzy wplyw ostatnich wartosci, przypisujac wagi danym z poszczegolnych okresow.\n",
    "- Wykladnicza srednia ruchoma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza skladowych glownych\n",
    "\n",
    "Analiza skladowych glownych jest najczesciej stosowana metoda automatycznej redukcji wymiarow. Jej celem jest zastapienie oryginalnych zmiennych mniejsza liczba nowych zmiennych, tracac jak najmniej informacji. Analiza skladowych glownych wykorzystuje znane nam pojecia korelacji i wariancji, a tytulowe skladowe glowne to liniowe kombinacje oryginalnych zmiennych.\n",
    "\n",
    "Skladowe glowne wylicza sie jako wektory wlasne macierzy kowariancji oryginalnych danych. Analiza PCA przebiega w czterech krokach:\n",
    "\n",
    "- policzenie sredniej i macierzy kowariancji zmiennych (maciezry zawierajacej wspolczynnik wariancji wszystkich zmiennych)\n",
    "- znalezienie wektorow wlasnych i ich wartosci wlasnych\n",
    "- wybor znalezionych wektorow wlasnych - wybiera sie kolejne wektory o najwiekrzych wlasnosciach wlasnych\n",
    "- projekcja oryginalnych punktow do przestrzeni wyznaczonej przez wybrane wektory wlasne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybor zmiennych przydatnych dla modelu\n",
    "\n",
    "Trzecia metoda redukcji danych polega na ocenie faktycznej przydatnosci zmiennych w modelu. Ta metoda jest najbardziej kosztowna, bo wymaga wielokrotnego trenowania modelu przy uzyciu roznych kombinacji zmiennych wejsciowych. Metoda ta wystepuje w dwoch wariantach:\n",
    "\n",
    "- Mozemy zaczac od jednej, najsilniej skorelowanej zmiennej z wyjsciowa, i do kolejnych modeli kandydujacych dodac po jednej z pozostalych zmiennych wejsciowych. Jakosc wzystkich modeli nauczonych na parach zmiennych wejsciowych porownuje sie i wybiera model najlepszy wedlug wybranego kryterium oceny. Nastepnie dodaje sie trzecia zmienna, znowu po kolei trenujac modele kandydujace na kombinacjach pozostalych zmiennych.\n",
    "- Mozemy tez zaczac od wszystkich zmiennych, a nastepnie z nauczonego modelu usunac jedna zmnienna i ponownie go wytrenowac. Zamiast usuwac zmienne, mozemy tez zastapic je zmiennymi losowymi.\n",
    "\n",
    "## Rownowazenie danych\n",
    "\n",
    "Kiedy nalezy zrownowazyc dane? To zalezy od danych, zastosowanego algorytmu uczenia maszynowego i wybranej funkcji kosztu. Jezeli rzadko wystepujacy stan zmiennej wyjsciowej jest latwo separowany, to model nauczy sie go rozpoznawac nawet na 10% przypadkow. Jesli jednka w danych treningowych wystepuja nakladajace sie na siebie, ale nalezace do roznych klas, na przyklad identyczne transakcje, z ktorych niektore byly proba oszustwa, a inne nie, nawet 30% przypadkow rzadziej wystepujacego stanu moze sie okazac za malo.\n",
    "\n",
    "Dane mozna rownowazyc na dwa sposoby, albo poprzez usuniecie czesci przypadkow wiekrzosciowych, albo poprzez nadprobkowanie czyli dodanie do zbioru treningowego dodatkowych przypadkow mniejszosciowych. Do nadprobkowania najczesciej uzywa sie algorytmu SMOTHE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-krotna walidacja krzyzowa\n",
    "\n",
    "W przypadku gdy mamy za malo danych aby podzielic je na dane treningowe oraz testowe mozemy uzyc metody $k$-krotnej walidacji krzyzowej. Przebiega ona nastepujaco:\n",
    "\n",
    "- Zbior danych zrodlowych jest losowo dzielony na $k$ mozliwie rownolicznych, rozlacznych partycji\n",
    "- tworzonych jest $k$ kopii ocenianego modelu eksploracji danych\n",
    "- kazda z tych kopii jest trenowana na podstawie $k-1$ partycji, a do oceny jej dokladnosci jest uzywana pozostala partycja - na przyklad pierwsza kopia modelu zostanie nauczona na partycjach od $S_1$ do $S_9$ i oceniana na podstawie partycji $S_{10}$, druga kopia zostanie nauczona na partycjacj od $S_2$ do $S_{10}$ i oceniana na podstawie partycji $S_1$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0394993702810e63cf4a2cb4306f486c4e50bf3299d5af3d2bf38c01a54e229"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
